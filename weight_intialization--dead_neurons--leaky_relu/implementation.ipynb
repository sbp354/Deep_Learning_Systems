{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import glorot_normal, normal\n",
    "from deepreplay.callbacks import ReplayData\n",
    "from deepreplay.replay import Replay\n",
    "from deepreplay.plot import compose_plots\n",
    "import keras.initializers\n",
    "from matplotlib import pyplot as plt\n",
    "import keras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.\n",
    "Explain vanishing gradients phenomenon using standard normalization with different values of standard deviation and tanh and sigmoid activation functions. Then show how Xavier (aka Glorot normal) initialization of weights helps in dealing with this problem. Next use ReLU activation and show that instead of Xavier initialization, He initialization works better for ReLU activation. You can plot activations at each of the 5 layers to answer this question."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_model(n_layers, input_dim, units, activation, initializer):\n",
    "    if isinstance(units, list):\n",
    "        assert len(units) == n_layers\n",
    "    else:\n",
    "        units = [units] * n_layers\n",
    "        \n",
    "    model = Sequential()\n",
    "    # Adds first hidden layer with input_dim parameter\n",
    "    model.add(Dense(units=units[0], \n",
    "                    input_dim=input_dim, \n",
    "                    activation=activation,\n",
    "                    kernel_initializer=initializer, \n",
    "                    name='h1'))\n",
    "    \n",
    "    # Adds remaining hidden layers\n",
    "    for i in range(2, n_layers + 1):\n",
    "        model.add(Dense(units=units[i-1], \n",
    "                        activation=activation, \n",
    "                        kernel_initializer=initializer, \n",
    "                        name='h{}'.format(i)))\n",
    "    \n",
    "    # Adds output layer\n",
    "    model.add(Dense(units=1, activation='sigmoid', kernel_initializer=initializer, name='o'))\n",
    "    # Compiles the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_initializations(filename, group_name, initializer, activation, output_root):\n",
    "        model = build_model(n_layers=5, input_dim=10, units=100, \n",
    "                    activation=activation, initializer=initializer)\n",
    "        # Since we only need initial weights, we don't even need to train the model! \n",
    "        # We still use the ReplayData callback, but we can pass the model as argument instead\n",
    "        h5_file = f'{filename}.h5'\n",
    "        replaydata = ReplayData(X, y, filename=h5_file, group_name=group_name, model=model)\n",
    "\n",
    "        # Now we feed the data to the actual Replay object\n",
    "        # so we can build the visualizations\n",
    "        replay = Replay(replay_filename=h5_file, group_name=group_name)\n",
    "\n",
    "        # Using subplot2grid to assemble a complex figure...\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        ax_zvalues = plt.subplot2grid((2, 2), (0, 0))\n",
    "        ax_weights = plt.subplot2grid((2, 2), (0, 1))\n",
    "        ax_activations = plt.subplot2grid((2, 2), (1, 0))\n",
    "        ax_gradients = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "        wv = replay.build_weights(ax_weights)\n",
    "        gv = replay.build_gradients(ax_gradients)\n",
    "        # Z-values\n",
    "        zv = replay.build_outputs(ax_zvalues, before_activation=True, \n",
    "                                exclude_outputs=True, include_inputs=False)\n",
    "        # Activations\n",
    "        av = replay.build_outputs(ax_activations, exclude_outputs=True, include_inputs=False)\n",
    "\n",
    "        # Finally, we use compose_plots to update all\n",
    "        # visualizations at once\n",
    "        fig = compose_plots([zv, wv, av, gv], \n",
    "                            epoch=0, \n",
    "                            title=f'Activation: {act} - Initializer: Normal $\\sigma = {std}$')\n",
    "        plt.savefig(os.path.join(output_root, f\"{filename}.png\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X, y = load_data()\n",
    "replaydata = ReplayData(X, y, filename='model_files/hyperparms_in_action.h5', group_name='part1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the majority of this notebook was run on Google co-lab but I needed to use Python 3.6 to get deepreplay to work so I actually ran the following plotting code on a local instance. Plots are included separately but the code below is what I ran to get them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plots_folder = 'plots'\n",
    "std_list = [0.01, 0.1, 1]\n",
    "norm_activations = ['tanh', 'sigmoid']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for act in norm_activations:\n",
    "    for std in std_list:\n",
    "        filename = f'normal_initializer_{act}_{std}'\n",
    "        group_name = f'normal_{act}_{std}'\n",
    "        # Uses normal initializer\n",
    "        init = keras.initializers.normal(mean=0, stddev=std, seed=12)\n",
    "        \n",
    "        plot_initializations(filename, group_name, init, act, output_root = os.getcwd())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "for act in norm_activations:\n",
    "    for std in std_list:  \n",
    "        display(Image.open(os.path.join(plots_folder, f'normal_initializer_{act}_{std}.png')))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/saraprice/Documents/NYU/Spring2022/DL_Systems/Deep_Learning_Systems/weight_intialization--dead_neurons--leaky_relu'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For sigmoid with std = 0.01, we see the gradient completely vanishes and activations and Z values are in too small of a range. When we increase to std = 0.1 we still see the vanishing gradient but the activations and Z-values do seem to be in a better range. With std = 1 we no longer have a vanishing gradient but the Z-values have too wide a range and activations are in \"binary mode\".\n",
    "\n",
    "For tanh with std = 0.01 we start with a vanished gradient at the last layer and Z-values and activations are in too small of ranges. For 0.1 we don't see vanishing gradient anymore and Z values and activations are in good ranges so this looks better. When we increase std too much to 1 though we see no vanishing vgradient but the binary activations and wide range of Z-values like with sigmoid."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Tanh and Sigmoid with Glorot Initialization**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for act in norm_activations:\n",
    "    filename = f'glorot_initializer_{act}'\n",
    "    group_name = f'glorot_{act}'\n",
    "\n",
    "    # Uses normal initializer\n",
    "    init = keras.initializers.glorot_normal(mean=0, stddev=std, seed=12)\n",
    "\n",
    "    plot_initializations(filename, group_name, init, act, output_root = os.getcwd())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for act in norm_activations:\n",
    "    display(Image.open(os.path.join(prob2_folder, f'glorot_initializer_{act}.png')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Glorot normalization clearly helps Tanh by preventing vanishing gradients and keep activiations in a good range. However, glorot normalization does not appear to help sigmoid activation so much because the gradient fully vanishes by the first layer. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **ReLU activation w/ Glorot normal initialization**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = f'glorot_initializer_relu'\n",
    "group_name = f'glorot_relu'\n",
    "\n",
    "# Uses normal initializer\n",
    "init = keras.initializers.glorot_normal(seed=12)\n",
    "\n",
    "plot_initializations(filename, group_name, init, 'relu', output_root = os.getcwd())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display(Image.open(os.path.join(plots_folder, f'glorot_initializer_relu.png')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that glorot normalization does not work as well for ReLU given the gradients essentially vanish/ are on a small scale and the activations don't get very high. Also we want the Z-values to be more uniform across all the layers.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **ReLU Activation w/ He Initialization**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = f'he_initializer_relu'\n",
    "group_name = f'he_relu'\n",
    "\n",
    "# Uses normal initializer\n",
    "init = keras.initializers.he_normal(seed=12)\n",
    "\n",
    "plot_initializations(filename, group_name, init, 'relu', output_root = os.path.join(os.getcwd(), plots_folder))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display(Image.open(os.path.join(prob2_folder, f'he_initializer_relu.png')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "He intitialization works better for ReLU because it keeps the gradients on a slighly larger scale and we see more uniformity across Z-values. Additionally the activation values are higher across the layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. \n",
    "The dying ReLU is a kind of vanishing gradient, which refers to a problem when ReLU neurons become\n",
    "inactive and only output 0 for any input. In the worst case of dying ReLU, ReLU neurons at a certain\n",
    "layer are all dead, i.e., the entire network dies and is referred as the dying ReLU neural networks in\n",
    "Lu et al (reference below). A dying ReLU neural network collapses to a constant function. Show this\n",
    "phenomenon using any one of the three 1-dimensional functions in page 13 of Lu et al. Use a 10-layer\n",
    "ReLU network with width 2 (hidden units per layer). Use minibatch of 64 and draw training data\n",
    "uniformly from $[\\sqrt{− 7}, \\sqrt{7}]$. Perform 1000 independent training simulations each with 3,000 training\n",
    "points. Out of these 1000 simulations, what fraction resulted in neural network collapse. Is your answer close to over 90% as was reported in Lu et al. ? (10)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    def __getitem__(self, idx):\n",
    "        target = self.target[idx]\n",
    "        data = self.data[idx]\n",
    "        #sample = {\"data\": data, \"label\": target}\n",
    "        return data, target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class relu_10_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1,2)\n",
    "        self.fc2 = nn.Linear(2,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "        #self.layers = nn.ModuleList([self.seq() for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        for i in range(10):\n",
    "            x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = relu_10_layer().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)\n",
    "model.train()\n",
    "np.random.seed(3)\n",
    "dead_network = 0\n",
    "total = 0\n",
    "for j in range(1000):\n",
    "    train_dset = CustomDataset(training_data[:,j],targets[:,j])\n",
    "    train_loader = torch.utils.data.DataLoader(train_dset, batch_size= 64)\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data = torch.unsqueeze(torch.tensor(data), dim = 1).float().to(device)\n",
    "        target = torch.tensor(target).float().to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        #print(torch.squeeze(output, dim =1))\n",
    "        loss = criterion(torch.squeeze(output, dim =1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if len(torch.unique(output))==1:\n",
    "            dead_network+=1\n",
    "        total+=1\n",
    "    \n",
    "    if (j%100 == 0) & (j!=0):\n",
    "        print(f\"After batch {j} % of dead networks: {dead_network/total*100}\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. \n",
    "Instead of ReLU consider Leaky ReLU activation as defined below:\n",
    "$$\\begin{equation}\n",
    "\\phi(x) = \n",
    "\\begin{cases}\n",
    "  z & \\text{if } z > 0 \\\\\n",
    "  0.01z & \\text{if } z \\leq 0\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "Run the 1000 training simulations in part 2 with Leaky ReLU activation and keeping everything else same. Again calculate the fraction of simulations that resulted in neural network collapse. Did Leaky ReLU help in preventing dying neurons ?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class leaky_relu_10_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1,2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        self.seq = nn.Sequential(nn.Linear(2,2), nn.LeakyReLU())\n",
    "        #self.layers = nn.ModuleList([self.seq() for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        for i in range(9):\n",
    "            x = self.seq(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = leaky_relu_10_layer().to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "model.train()\n",
    "dead_network_leaky = 0\n",
    "np.random.seed(4)\n",
    "total = 0\n",
    "for j in range(1000):\n",
    "    train_dset = CustomDataset(training_data[:,j],targets[:,j])\n",
    "    train_loader = torch.utils.data.DataLoader(train_dset, batch_size= 64)\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data = torch.unsqueeze(torch.tensor(data), dim = 1).float().to(device)\n",
    "        target = torch.tensor(target).float().to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(torch.squeeze(output,1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if len(torch.unique(output))==1:\n",
    "            dead_network_leaky+=1\n",
    "        total+=1\n",
    "    \n",
    "    \n",
    "    if (j%100 == 0) & (j!=0):\n",
    "        print(f\"After batch {j} % of dead networks: {dead_network_leaky/total*100}%\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"% of dead networks using Leaky ReLU: {dead_network_leaky/total*100}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Yes leaky relu helped with the dying neurons - a far smaller percentage of simulations collapsed"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}