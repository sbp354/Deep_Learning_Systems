{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from IPython.display import Image\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Up\n",
    "Consider a 2-dimensional data set in which all points with $x_1 > x_2$ belong to the positive class, and all points with x_1 ≤ x_2 belong to the negative class. Therefore, the true separator of the two classes is linear hyperplane (line) defined by $x_1 − x_2 = 0$. Now create a training data set with 10 points randomly generated inside the unit square in the positive quadrant. Label each point depending on whether or not the first coordinate x1 is greater than its second coordinate $x_2$. Now consider the following loss function for training pair $(\\bar{X} ,y)$ and weight vector $\\tilde{W}$:\n",
    "$$L = \\mathrm{max}\\{0, a-y(\\bar{W} \\cdot \\bar{X})\\},$$\n",
    "\n",
    "where the test instances are predicted as $\\hat{y} = sign{\\bar{W}\\cdot  \\bar{X}}$. For this problem, $\\bar{W}  = [w1,w2], \\bar{X } = [x1,x2]$ and $\\hat{y} = \\mathrm{sign}(w1x1 + w2x2)$. A value of a = 0 corresponds to the perceptron criterion and a value of $a = 1$ corresponds to hinge-loss."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. \n",
    "Implement the perceptron algorithm without regularization, train it on the 10 points above, and test its accuracy on 5000 randomly generated points inside the unit square. Generate the test points using the same procedure as the training points. You need to have your own implementation of the perceptron algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.random((10,2))\n",
    "X_test = np.random.random((5000,2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_Y(X):\n",
    "    Y = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        if X[i,0]> X[i,1]:\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = -1\n",
    "    return Y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Y = generate_Y(X)\n",
    "Y_test = generate_Y(X_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def predict_y(W, X):\n",
    "    return np.sign(X@W)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_perceptron(X, Y, loss, lr, stopping_criteria = 20000):\n",
    "    W = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    y_hat = np.zeros(X.shape[0])\n",
    "\n",
    "    if loss == 'perceptron_criterion':\n",
    "        while len(np.where(Y!=y_hat)[0])>0:\n",
    "            for i, x_i in enumerate(X):\n",
    "                y_hat[i] = predict_y(W, x_i)\n",
    "                if y_hat[i]!=Y[i]:\n",
    "                  W += lr*x_i*Y[i]\n",
    "\n",
    "    elif loss == 'hinge':\n",
    "        #Add in stopping criterion since hinge loss isn't gauranteed to converge\n",
    "        incorrect_tracker = np.zeros(len(X))\n",
    "        while len(np.where(np.repeat(1, len(X)) - Y*(X@W) > 0)[0]):\n",
    "            incorrect_count = len(np.where(np.repeat(1, len(X)) - Y*(X@W) > 0)[0])-1\n",
    "            incorrect_tracker[incorrect_count]+=1\n",
    "            if incorrect_tracker[incorrect_count]> stopping_criteria:\n",
    "              break\n",
    "            else:\n",
    "              \n",
    "              for i, x_i in enumerate(X):\n",
    "                  y_hat[i] = predict_y(W, x_i)\n",
    "                  if 1 - (x_i@W)*Y[i] > 0:\n",
    "                      W += lr*x_i*Y[i]\n",
    "    return W\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def eval_perceptron(W, X, Y):\n",
    "    Y_hat = np.sign(X@W)\n",
    "    acc = np.sum(Y_hat == Y)/len(Y)\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "W = train_perceptron(X, Y, loss = 'perceptron_criterion',lr = 0.1)\n",
    "test_acc = eval_perceptron(W, X_test, Y_test)\n",
    "print(W)\n",
    "print(\"Test accuracy with perceptron Criterion\", test_acc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(X[:,0][np.argwhere(Y>0)], X[:,1][np.argwhere(Y>0)])\n",
    "plt.scatter(X[:,0][np.argwhere(Y<=0)], X[:,1][np.argwhere(Y<=0)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.\n",
    "Change the perceptron criterion to hinge-loss in your implementation for training, and repeat the accuracy computation on the same test points above. Regularization is not used. In which case do you obtain better accuracy and why?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "W_hinge = train_perceptron(X, Y, loss = 'hinge', lr = 0.01)\n",
    "test_acc_hinge = eval_perceptron(W_hinge, X_test, Y_test)\n",
    "print(W_hinge)\n",
    "print(\"Test aaccuraccy with hinge loss:\",test_acc_hinge)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hinge loss gets better accuracy on the test set. This is because perceptron has infinitely many ways it can draw a separating hyperplane between the classes while hinge loss has the 1 margin it has to account for. The diagonal line where x = y is a natural split for this dataset that will always classify all point correctly. Because of the margin hinge loss is more likely to be forced closer to that true middle line whereas with the regular perceptron criterion there can be more variance.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. \n",
    "In which case do you think that the classification of the same 5000 test instances will not change significantly by using a different set of 10 training points?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Try different iterations of train set to test answer\n",
    "percep_higher = 0\n",
    "hinge_higher = 0\n",
    "for i in range(30):\n",
    "    np.random.seed(i)\n",
    "    print(f\"ITER {i}\")\n",
    "    X_train = np.random.random((10,2))\n",
    "    Y_train = generate_Y(X_train)\n",
    "    W_percep_crit = train_perceptron(X_train, Y_train, loss = 'perceptron_criterion',lr = 0.01)\n",
    "    test_acc_percep_crit = eval_perceptron(W_percep_crit, X_test, Y_test)\n",
    "    print(f\"Accuracy w/ perceptron criterion: {test_acc_percep_crit}\")\n",
    "\n",
    "    W_hinge_testing = train_perceptron(X_train, Y_train, loss = 'hinge',lr = 0.01)\n",
    "    test_acc_hinge = eval_perceptron(W_hinge_testing, X_test, Y_test)\n",
    "    print(f\"Accuracy w/ hinge loss: {test_acc_hinge}\")\n",
    "\n",
    "    if test_acc_percep_crit > test_acc_hinge:\n",
    "        percep_higher +=1\n",
    "    else:\n",
    "        hinge_higher+=1\n",
    "print(percep_higher)\n",
    "print(hinge_higher)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy with the perceptron criteriion will change more because there are infinitely many solutions that will separate the classes and the location can be more random depending on the training data/ order in which the algorithm goes through the training data. This could mean that some of the separating hyperplanes found in training with the perceptron criterion could be farther from the true middle line and thus perform worse on the test set. Hinge loss, because of the margin, does a better job of pushing the separating hyperplane closer to the middle. We see below in a simulation that sometimes accuracy will be worse when trained with hinge loss but perceptron criterion is worse for a majority of 30 random test cases tried below. However, I will note that hinge loss does do worse than perceptron a decent amount of the time."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}